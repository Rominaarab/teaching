{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "a8d7e3f0",
      "metadata": {
        "id": "a8d7e3f0"
      },
      "outputs": [],
      "source": [
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download required NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7i4Rvh2Fp2R5",
        "outputId": "f380d481-bb94-4fce-b032-17c85de66cea"
      },
      "id": "7i4Rvh2Fp2R5",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import NLTK modules\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer"
      ],
      "metadata": {
        "id": "Hg9Ez0Dcqiu_"
      },
      "id": "Hg9Ez0Dcqiu_",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample text\n",
        "text = \"This is an example sentence for demonstrating NLTK preprocessing techniques.\""
      ],
      "metadata": {
        "id": "wxahTgvXqnRL"
      },
      "id": "wxahTgvXqnRL",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "nltk.download('punkt_tab')\n",
        "print(\"Tokenization:\")\n",
        "print(word_tokenize(text))\n",
        "print(sent_tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zaUQ_jKgqqoN",
        "outputId": "dc9d7d57-f04c-4768-90c3-d9f5e52edbdd"
      },
      "id": "zaUQ_jKgqqoN",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenization:\n",
            "['This', 'is', 'an', 'example', 'sentence', 'for', 'demonstrating', 'NLTK', 'preprocessing', 'techniques', '.']\n",
            "['This is an example sentence for demonstrating NLTK preprocessing techniques.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stopwords removal\n",
        "print(\"\\nStopwords removal:\")\n",
        "stop_words = set(stopwords.words('english'))\n",
        "tokens = word_tokenize(text)\n",
        "filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
        "print(filtered_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfLRhqaSq56z",
        "outputId": "322bdd9c-ebaf-4751-e82d-8fe4ca3c0bcf"
      },
      "id": "KfLRhqaSq56z",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Stopwords removal:\n",
            "['example', 'sentence', 'demonstrating', 'NLTK', 'preprocessing', 'techniques', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stemming\n",
        "print(\"\\nStemming:\")\n",
        "stemmer = PorterStemmer()\n",
        "tokens = word_tokenize(text)\n",
        "stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
        "print(stemmed_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSYkf4MZrabJ",
        "outputId": "d8e32ab6-e058-474a-ebcf-b3210dee4c2a"
      },
      "id": "iSYkf4MZrabJ",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Stemming:\n",
            "['thi', 'is', 'an', 'exampl', 'sentenc', 'for', 'demonstr', 'nltk', 'preprocess', 'techniqu', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatization\n",
        "print(\"\\nLemmatization:\")\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "tokens = word_tokenize(text)\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "print(lemmatized_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x794oBM0rcgS",
        "outputId": "d8a73657-d477-4bce-a02c-a2607bbf5743"
      },
      "id": "x794oBM0rcgS",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Lemmatization:\n",
            "['This', 'is', 'an', 'example', 'sentence', 'for', 'demonstrating', 'NLTK', 'preprocessing', 'technique', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Part-of-speech (POS) tagging\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "print(\"\\nPOS tagging:\")\n",
        "tokens = word_tokenize(text)\n",
        "pos_tags = nltk.pos_tag(tokens)\n",
        "print(pos_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MinLoK76rhGx",
        "outputId": "3f9f87a1-b689-4cb2-a113-bdfcd9a0aff3"
      },
      "id": "MinLoK76rhGx",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "POS tagging:\n",
            "[('This', 'DT'), ('is', 'VBZ'), ('an', 'DT'), ('example', 'NN'), ('sentence', 'NN'), ('for', 'IN'), ('demonstrating', 'VBG'), ('NLTK', 'NNP'), ('preprocessing', 'VBG'), ('techniques', 'NNS'), ('.', '.')]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}