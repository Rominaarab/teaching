{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "a8d7e3f0",
      "metadata": {
        "id": "a8d7e3f0"
      },
      "outputs": [],
      "source": [
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download required NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7i4Rvh2Fp2R5",
        "outputId": "f380d481-bb94-4fce-b032-17c85de66cea"
      },
      "id": "7i4Rvh2Fp2R5",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import NLTK modules\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer"
      ],
      "metadata": {
        "id": "Hg9Ez0Dcqiu_"
      },
      "id": "Hg9Ez0Dcqiu_",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample text\n",
        "text = \"This is an example sentence for demonstrating NLTK preprocessing techniques.\""
      ],
      "metadata": {
        "id": "wxahTgvXqnRL"
      },
      "id": "wxahTgvXqnRL",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "nltk.download('punkt_tab')\n",
        "print(\"Tokenization:\")\n",
        "print(word_tokenize(text))\n",
        "print(sent_tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zaUQ_jKgqqoN",
        "outputId": "dc9d7d57-f04c-4768-90c3-d9f5e52edbdd"
      },
      "id": "zaUQ_jKgqqoN",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenization:\n",
            "['This', 'is', 'an', 'example', 'sentence', 'for', 'demonstrating', 'NLTK', 'preprocessing', 'techniques', '.']\n",
            "['This is an example sentence for demonstrating NLTK preprocessing techniques.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stopwords removal\n",
        "print(\"\\nStopwords removal:\")\n",
        "stop_words = set(stopwords.words('english'))\n",
        "tokens = word_tokenize(text)\n",
        "filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
        "print(filtered_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfLRhqaSq56z",
        "outputId": "322bdd9c-ebaf-4751-e82d-8fe4ca3c0bcf"
      },
      "id": "KfLRhqaSq56z",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Stopwords removal:\n",
            "['example', 'sentence', 'demonstrating', 'NLTK', 'preprocessing', 'techniques', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stemming\n",
        "print(\"\\nStemming:\")\n",
        "stemmer = PorterStemmer()\n",
        "tokens = word_tokenize(text)\n",
        "stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
        "print(stemmed_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSYkf4MZrabJ",
        "outputId": "d8e32ab6-e058-474a-ebcf-b3210dee4c2a"
      },
      "id": "iSYkf4MZrabJ",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Stemming:\n",
            "['thi', 'is', 'an', 'exampl', 'sentenc', 'for', 'demonstr', 'nltk', 'preprocess', 'techniqu', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatization\n",
        "print(\"\\nLemmatization:\")\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "tokens = word_tokenize(text)\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "print(lemmatized_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x794oBM0rcgS",
        "outputId": "d8a73657-d477-4bce-a02c-a2607bbf5743"
      },
      "id": "x794oBM0rcgS",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Lemmatization:\n",
            "['This', 'is', 'an', 'example', 'sentence', 'for', 'demonstrating', 'NLTK', 'preprocessing', 'technique', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Part-of-speech (POS) tagging\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "print(\"\\nPOS tagging:\")\n",
        "tokens = word_tokenize(text)\n",
        "pos_tags = nltk.pos_tag(tokens)\n",
        "print(pos_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MinLoK76rhGx",
        "outputId": "3f9f87a1-b689-4cb2-a113-bdfcd9a0aff3"
      },
      "id": "MinLoK76rhGx",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "POS tagging:\n",
            "[('This', 'DT'), ('is', 'VBZ'), ('an', 'DT'), ('example', 'NN'), ('sentence', 'NN'), ('for', 'IN'), ('demonstrating', 'VBG'), ('NLTK', 'NNP'), ('preprocessing', 'VBG'), ('techniques', 'NNS'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet"
      ],
      "metadata": {
        "id": "gm7PZLZ0xPe3"
      },
      "id": "gm7PZLZ0xPe3",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# Download WordNet if not already downloaded\n",
        "nltk.download('wordnet')'''"
      ],
      "metadata": {
        "id": "g34NjFDHzPJ9",
        "outputId": "ecb08bff-c3c8-40be-c2fa-8dc18543c03d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "id": "g34NjFDHzPJ9",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n# Download WordNet if not already downloaded\\nnltk.download('wordnet')\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word = \"Car\""
      ],
      "metadata": {
        "id": "sjBLYC9yxXIL"
      },
      "id": "sjBLYC9yxXIL",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "synonyms = set()\n",
        "for syn in wordnet.synsets(word):\n",
        "  for lemma in syn.lemmas():\n",
        "    synonyms.add(lemma.name())"
      ],
      "metadata": {
        "id": "a8-ReS-Nxfrs"
      },
      "id": "a8-ReS-Nxfrs",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"synonyms for'{}':\".format(word))\n",
        "for synonym in synonyms:\n",
        "  if synonym.lower() != word.lower():\n",
        "    print(synonym)"
      ],
      "metadata": {
        "id": "mLFxSK-Eyczm",
        "outputId": "7de4b35f-f780-4c01-ee32-a10d208d5b7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "mLFxSK-Eyczm",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "synonyms for'Car':\n",
            "auto\n",
            "elevator_car\n",
            "cable_car\n",
            "machine\n",
            "railroad_car\n",
            "gondola\n",
            "railway_car\n",
            "railcar\n",
            "automobile\n",
            "motorcar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random"
      ],
      "metadata": {
        "id": "BpCS7YeyzALC"
      },
      "id": "BpCS7YeyzALC",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "id": "ZVRTEvPFz4at",
        "outputId": "17ea8e57-b91a-4215-98c2-d71022d69575",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "ZVRTEvPFz4at",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### how can I knopw the synonym of a word \"car\" in google colab by the use of NLTK.Wordnet."
      ],
      "metadata": {
        "id": "XH67BBiuE7PO"
      },
      "id": "XH67BBiuE7PO"
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "import random\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "def replace_with_synonyms(sentence, n):\n",
        "    # Tokenize the sentence\n",
        "    tokens = nltk.word_tokenize(sentence)\n",
        "\n",
        "    # POS tag the tokens\n",
        "    pos_tags = nltk.pos_tag(tokens)\n",
        "\n",
        "    # Identify the words that can be replaced with synonyms\n",
        "    replaceable_words = [(i, word, pos) for i, (word, pos) in enumerate(pos_tags) if pos.startswith('NN') or pos.startswith('VB') or pos.startswith('JJ') or pos.startswith('RB')]\n",
        "\n",
        "    # Randomly select up to n words to replace\n",
        "    num_replacements = min(n, len(replaceable_words))\n",
        "    words_to_replace = random.sample(replaceable_words, num_replacements)\n",
        "\n",
        "    # Replace the selected words with their synonyms\n",
        "    for i, word, pos in words_to_replace:\n",
        "        synonyms = get_synonyms(word, pos)\n",
        "        if synonyms:\n",
        "            tokens[i] = random.choice(synonyms)\n",
        "\n",
        "    # Join the tokens back into a sentence\n",
        "    new_sentence = ' '.join(tokens)\n",
        "\n",
        "    return new_sentence\n",
        "\n",
        "def get_synonyms(word, pos):\n",
        "    # Map POS tags to WordNet POS tags\n",
        "    pos_map = {\n",
        "        'NN': 'n',\n",
        "        'VB': 'v',\n",
        "        'JJ': 'a',\n",
        "        'RB': 'r'\n",
        "    }\n",
        "\n",
        "    # Get the WordNet POS tag\n",
        "    wordnet_pos = pos_map.get(pos[:2])\n",
        "\n",
        "    if wordnet_pos:\n",
        "        synonyms = set()\n",
        "        for syn in wordnet.synsets(word, wordnet_pos):\n",
        "            for lemma in syn.lemmas():\n",
        "                synonyms.add(lemma.name())\n",
        "        # Remove the original word from the set of synonyms\n",
        "        synonyms.discard(word)\n",
        "        return list(synonyms)\n",
        "    else:\n",
        "        return []\n",
        "\n",
        "# Test the function\n",
        "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
        "n = 3\n",
        "print(replace_with_synonyms(sentence, n))"
      ],
      "metadata": {
        "id": "DG-3vmNV0GYj",
        "outputId": "83d36a18-0d7f-4801-bcce-570ff552841c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "DG-3vmNV0GYj",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The quick brown fox rise over the otiose frankfurter .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### I want to write a python function that takes a sentence and replace up to n random words with their synonyms using nltk wordnet"
      ],
      "metadata": {
        "id": "h5ZlF6ZFE0Tr"
      },
      "id": "h5ZlF6ZFE0Tr"
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet\n",
        "import random"
      ],
      "metadata": {
        "id": "pFCvd4uH1gL3"
      },
      "id": "pFCvd4uH1gL3",
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rep_syn(sentence, n):\n",
        "  tokens = nltk.word_tokenize(sentence)\n",
        "  pos_tags = nltk.pos_tag(tokens)\n",
        "  replaceable_words = [(i, word, pos) for i, (word, pos) in enumerate(pos_tags)\n",
        "  if pos.startswith('NN') or pos.startswith('VB') or pos.startswith('JJ') or pos.startswith('RB')]\n",
        "  # Randomly select up to n words to replace\n",
        "  num_replacements = min(n, len(replaceable_words))\n",
        "  words_to_replace = random.sample(replaceable_words, num_replacements)\n",
        "  for i, word, pos in words_to_replace:\n",
        "      synonyms = get_synonyms(word, pos)\n",
        "      if synonyms:\n",
        "          tokens[i] = random.choice(synonyms)\n",
        "\n",
        "  # Join the tokens back into a sentence\n",
        "  new_sentence = ' '.join(tokens)\n",
        "\n",
        "  return new_sentence"
      ],
      "metadata": {
        "id": "MnyyNYub4CrB"
      },
      "id": "MnyyNYub4CrB",
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_synonyms(word, pos):\n",
        "    # Map POS tags to WordNet POS tags\n",
        "    pos_map = {\n",
        "        'NN': 'n',\n",
        "        'VB': 'v',\n",
        "        'JJ': 'a',\n",
        "        'RB': 'r'\n",
        "    }\n",
        "\n",
        "    # Get the WordNet POS tag\n",
        "    wordnet_pos = pos_map.get(pos[:2])\n",
        "\n",
        "    if wordnet_pos:\n",
        "        synonyms = set()\n",
        "        for syn in wordnet.synsets(word, wordnet_pos):\n",
        "            for lemma in syn.lemmas():\n",
        "                synonyms.add(lemma.name())\n",
        "        # Remove the original word from the set of synonyms\n",
        "        synonyms.discard(word)\n",
        "        return list(synonyms)\n",
        "    else:\n",
        "        return []"
      ],
      "metadata": {
        "id": "UXMS_Csp8d7R"
      },
      "id": "UXMS_Csp8d7R",
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    sentence = input(\"Enter a sentence: \")\n",
        "    n = int(input(\"Enter the number of words to replace: \"))\n",
        "    new_sentence = replace_with_synonyms(sentence, n)\n",
        "    print(\"Modified sentence:\", new_sentence)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "5igVvbsJ9pGc",
        "outputId": "1e16df67-6c7d-4039-8d25-183936303ceb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "5igVvbsJ9pGc",
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a sentence: I love cars.\n",
            "Enter the number of words to replace: 2\n",
            "Modified sentence: I eff auto .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use spaCy's named entity recognition to identify entities in a sentence (such as people or locations) and replace them with randomly chosen alternatives of the same entity type."
      ],
      "metadata": {
        "id": "PlGgeCGhEtye"
      },
      "id": "PlGgeCGhEtye"
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import random"
      ],
      "metadata": {
        "id": "36FUd9iQ9p2r"
      },
      "id": "36FUd9iQ9p2r",
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "if you need to download :\n",
        "python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "9biAL9XN-OtV"
      },
      "id": "9biAL9XN-OtV"
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "LBpP_tTJ9ywL"
      },
      "id": "LBpP_tTJ9ywL",
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "entities = {\n",
        "    \"PERSON\": [\"John\", \"Jane\", \"Alice\", \"Bob\", \"Mike\"],\n",
        "    \"ORG\": [\"Google\", \"Amazon\", \"Microsoft\", \"Facebook\", \"Apple\"],\n",
        "    \"GPE\": [\"New York\", \"London\", \"Paris\", \"Tokyo\", \"Sydney\"],\n",
        "    \"LOC\": [\"Beach\", \"Mountain\", \"City\", \"Country\", \"Island\"]\n",
        "}"
      ],
      "metadata": {
        "id": "pLrVeHtK91j_"
      },
      "id": "pLrVeHtK91j_",
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_entities(sentence):\n",
        "    # Process the sentence using spaCy\n",
        "    doc = nlp(sentence)\n",
        "\n",
        "    # Identify entities in the sentence\n",
        "    entity_list = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "\n",
        "    # Replace entities with alternatives\n",
        "    new_sentence = sentence\n",
        "    for entity, label in entity_list:\n",
        "        alternatives = entities.get(label)\n",
        "        if alternatives:\n",
        "            new_entity = random.choice(alternatives)\n",
        "            new_sentence = new_sentence.replace(entity, new_entity)\n",
        "\n",
        "    return new_sentence"
      ],
      "metadata": {
        "id": "_6iyBxqI94A1"
      },
      "id": "_6iyBxqI94A1",
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    sentence = input(\"Enter a sentence: \")\n",
        "    new_sentence = replace_entities(sentence)\n",
        "    print(\"Modified sentence:\", new_sentence)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "zZ5ijQXK98zC",
        "outputId": "6066aeab-b91f-4f40-eb90-2b1058949356",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "zZ5ijQXK98zC",
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a sentence: I loce cars.\n",
            "Modified sentence: I loce cars.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Write a Python function that takes a sentence, translates it to French (or any other language), and then translates it back to English using the Google Translator from the deep_ translator library to create a paraphrased version."
      ],
      "metadata": {
        "id": "1QGnFBJiEhpk"
      },
      "id": "1QGnFBJiEhpk"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install deep_translator"
      ],
      "metadata": {
        "id": "YxbnbGFpDzZR",
        "outputId": "c51523fc-09d0-4763-df37-7681a90909e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "YxbnbGFpDzZR",
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting deep_translator\n",
            "  Downloading deep_translator-1.11.4-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.11/dist-packages (from deep_translator) (4.13.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from deep_translator) (2.32.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep_translator) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep_translator) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (2025.4.26)\n",
            "Downloading deep_translator-1.11.4-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: deep_translator\n",
            "Successfully installed deep_translator-1.11.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from deep_translator import GoogleTranslator"
      ],
      "metadata": {
        "id": "F7X0M1EG-ANO"
      },
      "id": "F7X0M1EG-ANO",
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def paraphrase_sentence(sentence, target_lang='fr'):\n",
        "    try:\n",
        "        # Translate the sentence to the target language\n",
        "        translated_sentence = GoogleTranslator(source='auto', target=target_lang).translate(sentence)\n",
        "\n",
        "        # Translate the translated sentence back to English\n",
        "        paraphrased_sentence = GoogleTranslator(source=target_lang, target='en').translate(translated_sentence)\n",
        "\n",
        "        return paraphrased_sentence\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return sentence"
      ],
      "metadata": {
        "id": "Mis36XfnCyka"
      },
      "id": "Mis36XfnCyka",
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    sentence = input(\"Enter a sentence: \")\n",
        "    paraphrased_sentence = paraphrase_sentence(sentence)\n",
        "    print(\"Paraphrased sentence:\", paraphrased_sentence)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "sSxWmPm0CtC1",
        "outputId": "2c22678a-3028-4fa5-af79-d9f175497e57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "sSxWmPm0CtC1",
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a sentence: I like no cars but I wanna work hard.\n",
            "Paraphrased sentence: I don't like cars but I want to work hard.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TcsNk3YCECAs"
      },
      "id": "TcsNk3YCECAs",
      "execution_count": 61,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}